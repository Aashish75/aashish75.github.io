<!DOCTYPE HTML>
<!-- 740 X 434
370 X 217  -->
<html>
	<head>
		<title>Aashish Mukund</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="apple-touch-icon" sizes="180x180" href="images/icons/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="images/icons/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="images/icons/favicon-16x16.png">
		<style>
			/* Reduce space between sections */
			#projects {
				margin-top: -280px;
			}

			/* Reduce line spacing in paragraphs */
			p {
				line-height: 1.1;
			}
		</style>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/profile_1.jpg" alt="" /></a>
					<h1><strong>Aashish Mukund</strong><br/>
						MSCS @ CU Boulder</h1>
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="one">
						<header class="major">
							<h2><b>Hello, I'm Aashish.</b></h2>
						</header>
						<p>I am currently pursuing a Masterâ€™s in Computer Science (MSCS) at the <a href="https://www.colorado.edu/">University of Colorado Boulder</a> and am actively seeking full-time opportunities in Machine Learning (ML) and Computer Vision (CV). 
						<br/><br/>
						
						In January 2025, I completed my tenure at the <a href="https://dannagurari.colorado.edu/research-group/">Image and Video Computing Group</a>, where I led novel efforts to identify how vision-language models reason during incorrect visual evidence on tasks like VQA. As part of this group, I also gained experience benchmarking proprietary and non-proprietary models, highlighting their performance gap in handling question ambiguity.
						<br/><br/>
						
						As a <a href="https://earthlab.colorado.edu/our-team/aashish-mukund">Graduate Research Assistant</a> at <a href="https://earthlab.colorado.edu/">Earth Lab</a>, I designed and developed fuel status prediction models (MESMA) for regions in Southern California by leveraging Sentinel-2 satellite data. 
						<br/><br/>

						These experiences, combined with my 2.5 years of full-time industry experience in productionizing large-scale virtual machines at <a href="https://tech.walmart.com/content/walmart-global-tech/en_us.html">Walmart Global Tech</a> have shaped me as an applied AI researcher & engineer. To learn more about my skill set, scroll down to explore some of my <b style="color: black;"><a href="#projects" style="text-decoration: none; color: black;">projects.</a></b>
						
						<br/><br/>
						When I'm not working on projects, you can find me diving deep in the ocean or at the top of a mountain. If you'd like a short break from tech, check out my collection of <b style="color: black;">
							<a href="certificates.html" style="text-decoration: none; color: black;">adventure certificates.</a>
						</b> <span> &#x1F605; </span>
						</p>
					</section>

				<!-- Two -->
					<section id="projects">
						<h2 style="color: #5f5d5d;">Personal Projects</h2>
						<div class="row">
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/blip-PathVQA-PEFT" target="_blank" class="image fit thumb"><img src="images/projects/results_blip.png" alt="emotic model" /></a>
								<h3>Fine-Tuning BLIP on PathVQA with LoRA</h3>
                                <p>Optimized BLIP for pathology-based Visual Question Answering (VQA) while reducing computational overhead using LoRA</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://aashish75.github.io/VisualisingAttentionMapsVQA/" target="_blank" class="image fit thumb"><img src="images/projects/att_viz.png" alt="CycleGAN" /></a>
								<h3>Attention Visualization as Evidence For VQA</h3>
								<p>Implemented a Grad-CAM based approach for visualizing attention in VQA. Backend app deployed in HF space.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/RAG-stablediffusion" target="_blank" class="image fit thumb"><img src="images/projects/RAG_1.png" alt="Quality Control using Deep-Learning" /></a>
								<h3>RAG+StableDiffusion</h3>
								<p>Enhancing Text-to-Image Generation with Retrieval-Augmented Diffusion Models</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/yolo-tensorrt-mlflow" target="_blank" class="image fit thumb"><img src="images/projects/nvidia_tensort.png" alt="Quality Control using Deep-Learning" /></a>
								<h3>YOLO+TensorRT+MLFlow</h3>
								<p>Comparing the performance of YOLOv5 PyTorch model vs TensorRT model and deploying on MLFlow</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/news-summarization" target="_blank" class="image fit thumb"><img src="images/projects/architecture.png" alt="Quality Control using Deep-Learning" /></a>
								<h3>News Summarization Service</h3>
								<p>Web app for news summarization built using ReactJS, Flask, GCP and S3.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/fine-tune-llama-trump" target="_blank" class="image fit thumb"><img src="images/projects/llama_trump.png" alt="FER" /></a>
								<h3>Tweet like Trump</h3>
								<p>Instruction-tuned the Llama-3.2-1B-Instruct model to improve it's ability to tweet like Donald Trump.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/wavenet-ASHA" target="_blank" class="image fit thumb"><img src="images/projects/wavenet_asha.png" alt="GAN" /></a>
								<h3>Text Generation with Wavenet</h3>
								<p>Optimised hyper-parameters for N-Gram text generation using ASHA (Adaptive Successive Halving Algorithm).</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://github.com/Aashish75/transformer-attention" target="_blank" class="image fit thumb"><img src="images/projects/seq_to_seq_att.png" alt="Quality Control using Deep-Learning" /></a>
								<h3>Seq-to-Seq Translation</h3>
								<p>Loosely implemented "Attention Is All You Need" paper for seq-to-seq translation</p>
							</article>
						</div>
						<ul class="actions">
							<li><a href="https://github.com/Aashish75?tab=repositories" target="_blank" class="button">All Projects</a></li>
						</ul>
					</section>
					
			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="MAILTO:aashishmukund@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<li><a href="https://github.com/Aashish75/" target="_blank" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/aashishm75/" target="_blank" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="files/resume.pdf" target="_blank" class="icon solid fa-file-alt"><span class="label">Resume</span></a></li>
					</ul>
					<ul class="copyright">
						<li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script>
				document.addEventListener("DOMContentLoaded", function () {
					document.querySelectorAll('a[href^="#"]').forEach(anchor => {
						anchor.addEventListener("click", function (e) {
							e.preventDefault();
							const target = document.querySelector(this.getAttribute("href"));
							if (target) {
								window.scrollTo({
									top: target.offsetTop - 20,
									behavior: "smooth"
								});
							}
						});
					});
				});
			</script>

	</body>
</html>
